# Конфигурация окружения WhisperLiveKit для GPU
# Оптимизировано для работы с NVIDIA GeForce RTX 4070
# Скопируйте этот файл в .env и измените значения в соответствии с вашими потребностями

# =================================================================
# КРИТИЧЕСКИ ВАЖНЫЕ НАСТРОЙКИ GPU - ДОЛЖНЫ БЫТЬ В НАЧАЛЕ!
# =================================================================

# ВКЛЮЧЕНИЕ CUDA ДЛЯ РАБОТЫ НА GPU
# Используем GPU для ускорения обработки
CUDA_VISIBLE_DEVICES=0
CUDA_LAUNCH_BLOCKING=1

# Использовать GPU для Whisper
WHISPER_CUDA=1
WHISPER_DEVICE=cuda

# Оптимизация потоков для GPU
# Увеличиваем количество потоков для лучшей производительности с GPU
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
OPENBLAS_NUM_THREADS=4

# Отключить предупреждения о неоптимальных путях
PYTHONWARNINGS=ignore::UserWarning
TORCH_WARN_LEVEL=0

# Отключить NNPACK warnings (библиотека не поддерживается на этом железе)
TORCH_USE_NNPACK=0
TORCH_USE_QNNPACK=0

# Отключить все предупреждения PyTorch
TORCH_SHOW_CPP_STACKTRACES=0
TORCH_USE_CUDA_DSA=0

# Дополнительные переменные для отключения NNPACK
TORCH_USE_NNPACK_ONLY=0
TORCH_USE_QNNPACK_ONLY=0
TORCH_USE_MKLDNN=0

# Принудительное использование GPU для faster-whisper
FASTER_WHISPER_CUDA=1
FASTER_WHISPER_DEVICE=cuda

# Дополнительные настройки для CUDA
CUDA_CACHE_DISABLE=0
CUDA_CACHE_MAXSIZE=2147483648

# =================================================================
# КОНФИГУРАЦИЯ СЕРВЕРА
# =================================================================

# Хост и порт сервера
# localhost - для локального доступа, 0.0.0.0 - для доступа извне
# Рекомендуется: localhost для разработки, 0.0.0.0 для продакшена
WLK_HOST=0.0.0.0

# Порт сервера (1024-65535)
# Рекомендуется: 8 000 для разработки, 80/443 для продакшена
WLK_PORT=8000

# SSL конфигурация (опционально, для HTTPS)
# Путь к SSL сертификату для HTTPS соединений
# WLK_SSL_CERTFILE=/path/to/certificate.pem
# Путь к приватному ключу SSL
# WLK_SSL_KEYFILE=/path/to/private_key.pem

# =================================================================
# КОНФИГУРАЦИЯ МОДЕЛИ
# =================================================================

# Размер модели Whisper
# tiny (~40MB, быстро, низкая точность) - для тестирования
# base (~150MB, баланс скорости и точности) - для обычного использования  
# small (~250MB, хорошая точность) - рекомендуется для большинства случаев
# medium (~770MB, высокая точность) - для высокого качества
# large-v3 (~1.5GB, максимальная точность) - для профессионального использования
# ОПТИМИЗИРОВАНО ДЛЯ GPU - small модель для лучшего баланса
WLK_MODEL=small

# Исходный язык ('auto' для автоматического определения)
# Поддерживаемые языки: en, de, fr, es, ru, ja, zh, it, pt, pl, tr, ko, ar, hi и др.
# auto - автоматическое определение (может быть неточным)
# Рекомендуется: указать конкретный язык для лучшей точности
WLK_LANGUAGE=ru

# Тип задачи
# transcribe - транскрипция на исходном языке
# translate - перевод на английский язык
# Рекомендуется: transcribe для обычной транскрипции
WLK_TASK=transcribe

# Бэкенд обработки
# simulstreaming - самый быстрый, низкая задержка (рекомендуется)
# faster-whisper - быстрый, оптимизированный для GPU
# whisper_timestamped - улучшенные временные метки
# mlx-whisper - оптимизация для Apple Silicon
# openai-api - использование OpenAI API
# ОПТИМИЗИРОВАНО ДЛЯ GPU - faster-whisper с CUDA поддержкой
WLK_BACKEND=faster-whisper

# Директория кэша моделей
# Укажите путь для сохранения загруженных моделей
# По умолчанию модели скачиваются в ~/.cache/huggingface и ~/.cache/whisper
# Рекомендуется: создать локальную папку models для быстрого доступа
WLK_MODEL_CACHE_DIR=./models

# Директория модели (переопределяет --model и --model_cache_dir)
# Используйте, если у вас уже есть загруженная модель в конкретной папке
# WLK_MODEL_DIR=./models/whisper-small

# =================================================================
# АВТОМАТИЧЕСКИ ЗАГРУЖАЕМЫЕ МОДЕЛИ
# =================================================================

# Whisper модели (автоматическая загрузка из OpenAI):
# ./models/whisper/
#   ├── tiny.pt (~40MB)
#   ├── base.pt (~150MB) 
#   ├── small.pt (~250MB)
#   ├── medium.pt (~770MB)
#   └── large-v3.pt (~1.5GB)

# Silero VAD модель (автоматическая загрузка):
# ./models/silero_vad/
#   └── silero_vad.onnx (~40MB)

# PyAnnote модели для диаризации (автоматическая загрузка через HF):
# ~/.cache/huggingface/hub/
#   ├── models--pyannote--segmentation-3.0/
#   └── models--speechbrain--spkrec-ecapa-voxceleb/

# =================================================================
# ДОПОЛНИТЕЛЬНЫЕ МОДЕЛИ (АВТОМАТИЧЕСКАЯ ЗАГРУЗКА)
# =================================================================

# CIF модели для SimulStreaming (автоматическая загрузка при установке):
# Скачиваются автоматически с: https://github.com/backspacetg/simul_whisper/tree/main/cif_models
# ./models/cif/
#   ├── cif_base.ckpt
#   ├── cif_small.ckpt
#   ├── cif_medium.ckpt
#   └── cif_large.ckpt (для large-v2)

# Если автоматическая загрузка не удалась, загрузите вручную:
# wget https://github.com/backspacetg/simul_whisper/raw/main/cif_models/cif_large.ckpt -O models/cif/cif_large.ckpt

# NeMo модели для Sortformer (автоматическая загрузка, если установлен):
# ~/.cache/torch/hub/
#   └── nvidia_models/

# =================================================================
# НАСТРОЙКА ПУТЕЙ МОДЕЛЕЙ
# =================================================================

# =================================================================
# КОНФИГУРАЦИЯ ОБРАБОТКИ АУДИО
# =================================================================

# Минимальный размер чанка аудио в секундах
# Меньше значение = быстрее отклик, но может снизить точность
# Больше значение = лучшая точность, но большая задержка
# Рекомендуется: 0.5 для баланса, 0.3 для скорости, 1.0 для точности
# ОПТИМИЗИРОВАНО ДЛЯ REAL-TIME - минимальный размер для мгновенного отклика
WLK_MIN_CHUNK_SIZE=0.5

# Обнаружение речевой активности (Voice Activity Detection)
# Отключить VAD (не рекомендуется, VAD экономит ресурсы)
# Рекомендуется: false (оставить VAD включённым)
# ВКЛЮЧЕНО ДЛЯ ОПТИМАЛЬНОЙ ПРОИЗВОДИТЕЛЬНОСТИ
WLK_NO_VAD=false

# Отключить контроллер речевой активности (Voice Activity Controller)
# Рекомендуется: false (оставить VAC включённым)
# ВКЛЮЧЕНО ДЛЯ ОПТИМАЛЬНОЙ ПРОИЗВОДИТЕЛЬНОСТИ
WLK_NO_VAC=false

# Размер чанка VAC в секундах
# Меньше значение = более частая проверка
# Рекомендуется: 0.04 (оптимальное значение)
# ОПТИМИЗИРОВАНО ДЛЯ REAL-TIME - более частая проверка для быстрого отклика
WLK_VAC_CHUNK_SIZE=0.04

# Файл для предварительного прогрева модели
# Помогает ускорить первую обработку
# Оставьте пустым для использования стандартного jfk.wav
# Укажите 'false' для отключения прогрева
WLK_WARMUP_FILE=./models/progrev/progrev.mp3

# Обрезка буфера
# segment - по сегментам Whisper (рекомендуется)
# sentence - по предложениям (требует сегментатор предложений)
# ОПТИМИЗИРОВАНО ДЛЯ REAL-TIME - используем sentence для мгновенного отображения
WLK_BUFFER_TRIMMING=segment

# Порог длины буфера в секундах для запуска обрезки
# ОПТИМИЗИРОВАНО ДЛЯ REAL-TIME - уменьшено для быстрого отображения
WLK_BUFFER_TRIMMING_SEC=5

# =================================================================
# КОНФИГУРАЦИЯ ДИАРИЗАЦИИ (ИДЕНТИФИКАЦИЯ ГОЛОСОВ)
# =================================================================

# Включить идентификацию голосов (диаризация)
# Позволяет различать разных говорящих в реальном времени
# Требует дополнительных вычислительных ресурсов
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
WLK_DIARIZATION=false

# Бэкенд диаризации (включено)
# diart - более старый, стабильный (требует HF токен)
# sortformer - новый, более точный (не работает с CUDA)
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
WLK_DIARIZATION_BACKEND=diart

# Модель сегментации для Diart (включено)
# Требует подтверждения лицензии на Hugging Face
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
WLK_SEGMENTATION_MODEL=pyannote/segmentation-3.0

# Модель эмбеддингов для Diart (включено)
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
WLK_EMBEDDING_MODEL=speechbrain/spkrec-ecapa-voxceleb

# Hugging Face токен (включено)
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
HUGGINGFACE_HUB_TOKEN=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

# Использовать пунктуацию (включено)
# ВКЛЮЧЕНО ДЛЯ РАБОТЫ С ДИАРИЗАЦИЕЙ
WLK_PUNCTUATION_SPLIT=true

# =================================================================
# НАСТРОЙКИ SIMULSTREAMING БЭКЕНДА
# =================================================================

# Отключить быстрый энкодер (Faster Whisper/MLX Whisper) для кодирования
# Включить, если есть ограничения по памяти GPU или проблемы с CUDA
# ОТКЛЮЧЕНО ДЛЯ РАБОТЫ С GPU
WLK_DISABLE_FAST_ENCODER=false

# Порог AlignAtt frame (меньше = быстрее, больше = точнее)
# Ключевой параметр для баланса скорости и точности
# 15-20 = очень быстро, 25-30 = баланс, 35-50 = высокая точность
# Рекомендуется: 25 для баланса, 20 для скорости, 30 для точности
# ОПТИМИЗИРОВАНО ДЛЯ REAL-TIME - минимальный порог для мгновенного отклика
WLK_FRAME_THRESHOLD=25

# Количество лучей для beam search (1 = greedy decoding)
# Больше лучей = лучшая точность, но медленнее
# Рекомендуется: 1 для скорости, 3-5 для точности
# ОПТИМИЗИРОВАНО ДЛЯ БЫСТРОЙ ТРАНСКРИПЦИИ
WLK_BEAMS=3

# Принудительно указать тип декодера (оставьте пустым для автовыбора)
# beam - beam search (медленнее, точнее)
# greedy - жадный алгоритм (быстрее)
# Рекомендуется: оставить пустым для автовыбора
WLK_DECODER_TYPE=

# Максимальная длина аудио буфера в секундах
# Ограничивает максимальную длину обрабатываемого аудио
# Рекомендуется: 30 для баланса, 20 для экономии памяти
# ОПТИМИЗИРОВАНО ДЛЯ GPU - больше буфер для лучшего качества
WLK_AUDIO_MAX_LEN=30

# Минимальная длина аудио для обработки в секундах
# Пропустить обработку, если аудио короче
# Полезно, когда --min-chunk-size мало
# Рекомендуется: 0.0 (отключено)
# ОПТИМИЗИРОВАНО ДЛЯ БЫСТРОЙ ОБРАБОТКИ
WLK_AUDIO_MIN_LEN=0.0

# Путь к модели CIF для обнаружения конца слов
# Помогает определять, когда обрезать неполные слова
# Модели доступны на: https://github.com/backspacetg/simul_whisper/tree/main/cif_models
# Автоматически используется соответствующая модель для small
# Оставить пустым, если CIF модель недоступна
# WLK_CIF_CKPT_PATH=./models/cif/small.pt
# WLK_CIF_CKPT_PATH=

# Никогда не обрезать неполные слова (переопределяет CIF модель)
# true - никогда не обрезать последнее слово
# false - использовать CIF модель для решения
# Рекомендуется: true (если CIF модель недоступна)
WLK_NEVER_FIRE=true

# Начальный промпт для модели
# Помогает настроить стиль транскрипции
# Промпт должен быть на целевом языке
# Примеры промптов:
# Общие: "Привет, как дела?", "Поговорим о бизнесе", "Обсудим проект"
# Медицина: "Медицинская консультация, анамнез, симптомы, диагноз"
# Образование: "Лекция по математике, формулы, теоремы"
# Право: "Юридическая консультация, законодательство, права граждан"
# Техника: "Техническое совещание, программирование, алгоритмы"
# ОПТИМИЗИРОВАНО ДЛЯ РУССКОЙ РЕЧИ
WLK_INIT_PROMPT="Привет! Говорите четко и медленно."

# Статичный промпт (не прокручивается)
# Может содержать терминологию, актуальную для всего документа
# Примеры статичных промптов:
# Медицина: "Медицинская консультация. Термины: анамнез, диагноз, симптомы, лечение"
# Юриспруденция: "Юридическая консультация. Указы, постановления, кодексы"
# Образование: "Образовательная лекция. Математика, физика, химия"
# Бизнес: "Деловое совещание. KPI, ROI, маркетинг, продажи"
# ОПТИМИЗИРОВАНО ДЛЯ ТОЧНОГО РАСПОЗНАВАНИЯ РУССКОЙ РЕЧИ
WLK_STATIC_INIT_PROMPT="Русская речь. Четкое произношение."

# Максимальное количество контекстных токенов
# Ограничивает количество предыдущего текста, используемого моделью
# Рекомендуется: оставить по умолчанию
# WLK_MAX_CONTEXT_TOKENS=

# Прямой путь к .pt файлу модели SimulStreaming
# Переопределяет --model для SimulStreaming
# WLK_MODEL_PATH=/path/to/model.pt

# Количество моделей для предварительной загрузки в память
# Помогает ускорить загрузку для множественных пользователей
# Установите равным ожидаемому количеству одновременных пользователей
# Рекомендуется: 1 для одиночного использования
WLK_PRELOADED_MODEL_COUNT=1

# =================================================================
# КОНФИГУРАЦИЯ GUNICORN (ПРОДАКШЕН)
# =================================================================

# Количество рабочих процессов (worker processes)
# Рекомендуемая формула: 2 * CPU_CORES + 1
# Пример: для 4-ядерного CPU = 2 * 4 + 1 = 9 workers
# Минимум: 2, Максимум: 12 (ограничено памятью моделей)
WLK_GUNICORN_WORKERS=8

# Класс worker'a (обязательно для WebSocket)
# uvicorn.workers.UvicornWorker - обязательно для async/WebSocket приложений
WLK_GUNICORN_WORKER_CLASS=uvicorn.workers.UvicornWorker

# Таймаут для worker'ов в секундах
# Увеличьте для больших моделей (загрузка может занять время)
# Рекомендуется: 120 для small, 300 для large моделей
WLK_GUNICORN_TIMEOUT=120

# Максимальное количество одновременных соединений на worker
# Ограничивает нагрузку на каждый процесс
# Рекомендуется: 1000 для обычных серверов
WLK_GUNICORN_MAX_REQUESTS=1000

# Количество слушающих сокетов (backlog)
# Рекомендуется: 2048 для высоконагруженных систем
WLK_GUNICORN_BACKLOG=2048

# Включить режим preload (загрузка приложения до создания workers)
# Уменьшает потребление памяти, но может усложнить отладку
# Рекомендуется: true для продакшена
WLK_GUNICORN_PRELOAD=true

# =================================================================
# МОНИТОРИНГ И GRACEFUL RESTART
# =================================================================

# Максимальное время ожидания graceful shutdown (секунды)
# Время на завершение текущих запросов
WLK_GUNICORN_GRACEFUL_TIMEOUT=30

# Путь к PID файлу (pid file)
# Нужен для контроля процесса и graceful restart
WLK_GUNICORN_PIDFILE=./gunicorn.pid

# Путь к файлу логов Gunicorn
# Отдельно от логов приложения
WLK_GUNICORN_ACCESS_LOG=./logs/gunicorn_access.log
WLK_GUNICORN_ERROR_LOG=./logs/gunicorn_error.log

# Пример команды запуска с Gunicorn:
# gunicorn -c gunicorn.conf.py whisperlivekit.basic_server:app
# или через start.sh с параметром --production

# Валидация через оценку уверенности (быстрее, но менее точная пунктуация)
# Ускоряет валидацию токенов, но может снизить качество пунктуации
# Рекомендуется: false для лучшего качества
# ВКЛЮЧЕНО ДЛЯ АДАПТИВНОЙ БУФЕРИЗАЦИИ - позволяет корректировать текст в реальном времени
WLK_CONFIDENCE_VALIDATION=true

# Отключить транскрипцию (оставить только диаризацию)
# Полезно для тестирования только системы диаризации
# Рекомендуется: false (оставить транскрипцию включённой)
WLK_NO_TRANSCRIPTION=false

# Уровень логирования
# DEBUG - подробная информация для отладки
# INFO - общая информация (рекомендуется)
# WARNING - только предупреждения и ошибки
# ERROR - только ошибки
# CRITICAL - только критические ошибки
# Рекомендуется: INFO для обычного использования, WARNING для продакшена
# ОПТИМИЗИРОВАНО ДЛЯ ЧИСТОГО ВЫВОДА
WLK_LOG_LEVEL=WARNING

# =================================================================
# НАСТРОЙКИ ДЛЯ REAL-TIME ОТОБРАЖЕНИЯ
# =================================================================

# Принудительный flush буфера каждые N секунд (для мгновенного отображения)
# 0 = отключено, 1-5 = рекомендуемые значения
WLK_FORCE_FLUSH_INTERVAL=1

# Минимальная длина текста для отправки (в символах)
# 0 = отправлять все, 1-10 = минимальная длина
WLK_MIN_TEXT_LENGTH=0

# Отправлять промежуточные результаты (неполные слова)
# true = отправлять все, false = только полные слова
WLK_SEND_PARTIAL_RESULTS=true

# =================================================================
# НАСТРОЙКИ АДАПТИВНОЙ БУФЕРИЗАЦИИ И КОНТЕКСТНОЙ КОРРЕКЦИИ
# =================================================================

# Максимальное количество контекстных токенов для коррекции
# Больше значение = лучшая коррекция, но больше задержка
# 0 = отключено, 50-200 = рекомендуемые значения
WLK_MAX_CONTEXT_TOKENS=100

# Включить контекстную коррекцию текста в реальном времени
# true = корректировать предыдущие слова на основе контекста
# false = отключить коррекцию
WLK_ENABLE_CONTEXT_CORRECTION=true

# Интервал коррекции в секундах
# Как часто проверять и корректировать предыдущий текст
WLK_CORRECTION_INTERVAL=0.5

# Минимальная уверенность для коррекции (0.0-1.0)
# Только токены с уверенностью выше этого порога будут корректироваться
WLK_CORRECTION_CONFIDENCE_THRESHOLD=0.7

# Включить автоматическую пунктуацию
# true = добавлять знаки препинания на основе контекста
# false = отключить автоматическую пунктуацию
WLK_AUTO_PUNCTUATION=true

# Включить коррекцию грамматики
# true = исправлять грамматические ошибки на основе контекста
# false = отключить коррекцию грамматики
WLK_GRAMMAR_CORRECTION=true

# =================================================================
# НАСТРОЙКИ АДАПТИВНОГО СЕРВЕРА
# =================================================================

# Включить адаптивную буферизацию
# true = использовать двойную буферизацию с контекстной коррекцией
# false = использовать стандартную буферизацию
WLK_ENABLE_ADAPTIVE_BUFFERING=false

# Использовать адаптивный сервер
# true = использовать adaptive_basic_server.py
# false = использовать стандартный basic_server.py
WLK_USE_ADAPTIVE_SERVER=false

# Endpoint для адаптивной транскрипции
# /asr - стандартный endpoint
# /asr-adaptive - адаптивный endpoint с контекстной коррекцией
WLK_ADAPTIVE_ENDPOINT=/asr-adaptive

# =================================================================
# ОПТИМИЗАЦИЯ ПРОИЗВОДИТЕЛЬНОСТИ
# =================================================================

# =================================================================
# ПРИМЕРЫ КОНФИГУРАЦИЙ
# =================================================================

# Пример 1: Высококачественная русская транскрипция с идентификацией голосов
# WLK_MODEL=small
# WLK_LANGUAGE=ru
# WLK_DIARIZATION=true
# # WLK_DIARIZATION_BACKEND=diart
# WLK_FRAME_THRESHOLD=30
# WLK_BEAMS=1

# Пример 2: Быстрая мультиязычная транскрипция
# WLK_MODEL=medium
# WLK_LANGUAGE=auto
# WLK_BACKEND=faster-whisper
# WLK_MIN_CHUNK_SIZE=0.3
# WLK_FRAME_THRESHOLD=20

# Пример 3: Продакшен сервер с HTTPS
# WLK_HOST=0.0.0.0
# WLK_PORT=443
# WLK_SSL_CERTFILE=/etc/ssl/certs/whisperlivekit.pem
# WLK_SSL_KEYFILE=/etc/ssl/private/whisperlivekit.key
# WLK_MODEL=small
# WLK_LOG_LEVEL=WARNING
# WLK_DIARIZATION=true

# Пример 4: Перевод на английский
# WLK_TASK=translate
# WLK_LANGUAGE=ru
# WLK_MODEL=medium

# Пример 5: Экономия GPU памяти
# WLK_MODEL=small
# WLK_DISABLE_FAST_ENCODER=false
# WLK_AUDIO_MAX_LEN=10.0
# WLK_DIARIZATION=false

# =================================================================
# ВАЖНЫЕ ПРИМЕЧАНИЯ
# =================================================================

# 1. Для диаризации с Diart backend:
#    - Подтвердите лицензии pyannote моделей на Hugging Face
#    - Выполните вход: huggingface-cli login
#    - Укажите валидный HUGGINGFACE_HUB_TOKEN

# 2. FFmpeg должен быть установлен и доступен в PATH

# 3. Для GPU ускорения установите соответствующую версию PyTorch с CUDA

# 4. Размеры моделей и требования:
#    - tiny: ~40 MB, быстро, но низкая точность
#    - base: ~150 MB, хороший баланс
#    - small: ~250 MB, хорошая точность (рекомендуется)
#    - medium: ~770 MB, высокая точность
#    - large-v3: ~1.5 GB, максимальная точность

# 5. Для продакшен развёртывания:
#    - Используйте reverse proxy (nginx) для статических файлов
#    - Установите соответствующие уровни логирования
#    - Настройте SSL сертификаты
#    - Рассмотрите использование множественных рабочих процессов с gunicorn

# 6. Оптимизация производительности:
#    - Для реального времени: WLK_FRAME_THRESHOLD=20, WLK_MIN_CHUNK_SIZE=0.3
#    - Для точности: WLK_FRAME_THRESHOLD=30, WLK_BEAMS=1
#    - Для экономии памяти: WLK_DISABLE_FAST_ENCODER=false